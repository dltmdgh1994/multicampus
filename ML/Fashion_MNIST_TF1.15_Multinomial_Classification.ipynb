{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/s_csmscox/jupyterSave/fashion-mnist_train.csv')\n",
    "test = pd.read_csv('C:/Users/s_csmscox/jupyterSave/fashion-mnist_test.csv')\n",
    "\n",
    "train_label = train['label']\n",
    "train.drop(['label'], axis=1, inplace=True)\n",
    "\n",
    "test_label = test['label']\n",
    "test.drop(['label'], axis=1, inplace=True)\n",
    "\n",
    "### 정규화\n",
    "scaler_train = MinMaxScaler()\n",
    "scaler_train.fit(train)\n",
    "norm_train_x = scaler_train.transform(train)\n",
    "\n",
    "scaler_test = MinMaxScaler()\n",
    "scaler_test.fit(train)\n",
    "norm_test_x = scaler_test.transform(test)\n",
    "\n",
    "### tensorflow 기능을 이용해서 one hot encoding을 생성\n",
    "sess = tf.Session()\n",
    "onehot_train_label = sess.run(tf.one_hot(train_label, depth=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([784,10]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([10]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logit)  # softmax activation function\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,\n",
    "                                                                 labels=T))\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "# parameter\n",
    "num_of_epoch = 2000\n",
    "batch_size = 10000\n",
    "\n",
    "# 학습용 함수\n",
    "def run_train(sess,train_x, train_t):\n",
    "    print('### 학습 시작 ###')\n",
    "    # 초기화\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(num_of_epoch):\n",
    "        total_batch = int(train_x.shape[0] / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_t = train_t[i*batch_size:(i+1)*batch_size]           \n",
    "            _, loss_val = sess.run([train,loss],\n",
    "                                   feed_dict={X: batch_x, T: batch_t})\n",
    "            \n",
    "        if step % 100 == 0:\n",
    "            print('Loss : {}'.format(loss_val))\n",
    "    print('### 학습 종료 ###')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 학습 시작 ###\n",
      "Loss : 12.563332557678223\n",
      "Loss : 1.7948132753372192\n",
      "Loss : 1.411192536354065\n",
      "Loss : 1.236683964729309\n",
      "Loss : 1.129176378250122\n",
      "Loss : 1.0531063079833984\n",
      "Loss : 0.994915246963501\n",
      "Loss : 0.9481379985809326\n",
      "Loss : 0.9092431664466858\n",
      "Loss : 0.8761132955551147\n",
      "Loss : 0.8473778367042542\n",
      "Loss : 0.8220950961112976\n",
      "Loss : 0.7995910048484802\n",
      "Loss : 0.7793689966201782\n",
      "Loss : 0.7610540986061096\n",
      "Loss : 0.7443560361862183\n",
      "Loss : 0.7290467023849487\n",
      "Loss : 0.7149415016174316\n",
      "Loss : 0.7018910050392151\n",
      "Loss : 0.6897708177566528\n",
      "### 학습 종료 ###\n",
      "측정한 각각의 결과값 : 0.8063333630561829\n",
      "### 학습 시작 ###\n",
      "Loss : 14.670181274414062\n",
      "Loss : 1.7868245840072632\n",
      "Loss : 1.4194927215576172\n",
      "Loss : 1.2500920295715332\n",
      "Loss : 1.1421419382095337\n",
      "Loss : 1.064643383026123\n",
      "Loss : 1.0049124956130981\n",
      "Loss : 0.9567506909370422\n",
      "Loss : 0.9167492985725403\n",
      "Loss : 0.8827683329582214\n",
      "Loss : 0.8533787131309509\n",
      "Loss : 0.8275905251502991\n",
      "Loss : 0.8046970963478088\n",
      "Loss : 0.7841760516166687\n",
      "Loss : 0.7656320333480835\n",
      "Loss : 0.748758852481842\n",
      "Loss : 0.733313798904419\n",
      "Loss : 0.7191025018692017\n",
      "Loss : 0.7059667706489563\n",
      "Loss : 0.6937762498855591\n",
      "### 학습 종료 ###\n",
      "측정한 각각의 결과값 : 0.8063333630561829\n",
      "### 학습 시작 ###\n",
      "Loss : 17.494312286376953\n",
      "Loss : 1.7850265502929688\n",
      "Loss : 1.392075538635254\n",
      "Loss : 1.2103145122528076\n",
      "Loss : 1.0993536710739136\n",
      "Loss : 1.0220835208892822\n",
      "Loss : 0.96381014585495\n",
      "Loss : 0.9175018668174744\n",
      "Loss : 0.8793433308601379\n",
      "Loss : 0.8470525145530701\n",
      "Loss : 0.8191949129104614\n",
      "Loss : 0.7948169112205505\n",
      "Loss : 0.7732433080673218\n",
      "Loss : 0.7539722919464111\n",
      "Loss : 0.7366193532943726\n",
      "Loss : 0.72088223695755\n",
      "Loss : 0.7065205574035645\n",
      "Loss : 0.6933402419090271\n",
      "Loss : 0.6811858415603638\n",
      "Loss : 0.669930100440979\n",
      "### 학습 종료 ###\n",
      "측정한 각각의 결과값 : 0.8032500147819519\n",
      "### 학습 시작 ###\n",
      "Loss : 12.833293914794922\n",
      "Loss : 1.749043345451355\n",
      "Loss : 1.3876756429672241\n",
      "Loss : 1.2151914834976196\n",
      "Loss : 1.1079347133636475\n",
      "Loss : 1.0327080488204956\n",
      "Loss : 0.9760029315948486\n",
      "Loss : 0.9310306906700134\n",
      "Loss : 0.893973171710968\n",
      "Loss : 0.862571120262146\n",
      "Loss : 0.8354203104972839\n",
      "Loss : 0.8115923404693604\n",
      "Loss : 0.7904389500617981\n",
      "Loss : 0.7714881300926208\n",
      "Loss : 0.754382312297821\n",
      "Loss : 0.7388409972190857\n",
      "Loss : 0.7246372103691101\n",
      "Loss : 0.7115830779075623\n",
      "Loss : 0.6995235085487366\n",
      "Loss : 0.6883283853530884\n",
      "### 학습 종료 ###\n",
      "측정한 각각의 결과값 : 0.7990000247955322\n",
      "### 학습 시작 ###\n",
      "Loss : 9.178894996643066\n",
      "Loss : 1.7844070196151733\n",
      "Loss : 1.4053542613983154\n",
      "Loss : 1.2359896898269653\n",
      "Loss : 1.1289918422698975\n",
      "Loss : 1.0517537593841553\n",
      "Loss : 0.9918572306632996\n",
      "Loss : 0.9432052969932556\n",
      "Loss : 0.902510941028595\n",
      "Loss : 0.8677999973297119\n",
      "Loss : 0.8377619385719299\n",
      "Loss : 0.8114628791809082\n",
      "Loss : 0.7882088422775269\n",
      "Loss : 0.7674672603607178\n",
      "Loss : 0.7488226294517517\n",
      "Loss : 0.7319461107254028\n",
      "Loss : 0.716575026512146\n",
      "Loss : 0.7024987936019897\n",
      "Loss : 0.6895453333854675\n",
      "Loss : 0.6775730848312378\n",
      "### 학습 종료 ###\n",
      "측정한 각각의 결과값 : 0.8019999861717224\n",
      "최종 K-Fold 교차검증을 사용한 Accuracy : 0.8033833503723145\n"
     ]
    }
   ],
   "source": [
    "# Accuracy    \n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "cv = 5          # Fold의 수\n",
    "results = []    \n",
    "                \n",
    "kf = KFold(n_splits=cv, shuffle=True) \n",
    "\n",
    "for training_idx, validation_idx in kf.split(norm_train_x):\n",
    "    training_x = norm_train_x[training_idx] # Fancy indexing\n",
    "    training_t = onehot_train_label[training_idx]\n",
    "    \n",
    "    val_x = norm_train_x[validation_idx]\n",
    "    val_t = onehot_train_label[validation_idx]\n",
    "    \n",
    "    run_train(sess,training_x,training_t)\n",
    "    acc = sess.run(accuracy, feed_dict={X:val_x, T:val_t})\n",
    "    print('측정한 각각의 결과값 : {}'.format(acc))\n",
    "    results.append(acc)\n",
    "\n",
    "\n",
    "print('최종 K-Fold 교차검증을 사용한 Accuracy : {}'.format(np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 값 : 7\n"
     ]
    }
   ],
   "source": [
    "# 공식을 이용해서 직접 img를 흑백처리\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = np.asarray(Image.open('C:/Users/s_csmscox/jupyterSave/8.png'))\n",
    "\n",
    "# 그레이 스케일링\n",
    "r = 0.2989\n",
    "g = 0.5870\n",
    "b = 0.1140\n",
    "gray = img[:, :, 0] * r + img[:, :, 1] * g + img[:, :, 2] * b\n",
    "img = Image.fromarray(gray)\n",
    "\n",
    "# 사이즈 조절\n",
    "img = img.resize((28,28))\n",
    "\n",
    "# 예측\n",
    "img = np.asarray(img)\n",
    "plt.imshow(img, cmap='gray') # cmap='gray_r의 경우 흑백 반전 = Greys'\n",
    "plt.show()\n",
    "\n",
    "img = 255 - img\n",
    "norm_img = scaler_test.transform(img.reshape(1,-1))\n",
    "\n",
    "result = sess.run(H, feed_dict={X: norm_img})\n",
    "\n",
    "for i in result:\n",
    "    m = i.max()\n",
    "    for j in range(10):\n",
    "        if i[j] == m:\n",
    "            print(\"예측 값 : {}\".format(j))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8082\n"
     ]
    }
   ],
   "source": [
    "result = sess.run(H, feed_dict={X: norm_test_x})\n",
    "pred = []\n",
    "\n",
    "for i in result:\n",
    "    m = i.max()\n",
    "    for j in range(10):\n",
    "        if i[j] == m:\n",
    "            pred.append(j)\n",
    "            break\n",
    "\n",
    "cnt = 0\n",
    "for predict, answer in zip(pred, test_label):\n",
    "    if predict == answer:\n",
    "        cnt += 1\n",
    "\n",
    "print(\"Accuracy : {}\".format((cnt/len(pred))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
